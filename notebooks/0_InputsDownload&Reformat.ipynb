{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download\n",
    "This Notebook downloads data needed to run the workflow steps 1 ot 5 for a test basin (in Canada or in the USA)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules, paths, variables & functions\n",
    "Paths & variables are the only elements you should need to modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 15:57:31,338 - root - INFO - Logging setup complete. Log file: /Users/drc858/GitHub/data_driven_forecasting_workflow/logs/data_driven_forecasting_20230815_155731.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import general required modules\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime,date\n",
    "from pprint import pprint\n",
    "\n",
    "# Add scripts to the system path\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "# Set up logging, configured for this workflow (see utilities.py)\n",
    "from utilities import setup_logging, read_settings\n",
    "setup_logging()\n",
    "# Set up logging for this notebook\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 15:57:33,904 - root - INFO - Settings logged from ../settings/config_North_America.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SWE_obs_path': '/Users/lla068/Documents/data_driven_forecasting/data/snow_obs/North_America/NorAmSnow_1979_2022.nc',\n",
      " 'basins_shp_path': '/Users/lla068/Documents/data_driven_forecasting/basins/North_America/NorAm_unregulated_basins.shp',\n",
      " 'domain': 'North_America',\n",
      " 'nival_glacial_basins_shp_path': '/Users/lla068/Documents/data_driven_forecasting/scripts/HPC_workflow/output_data/North_America/v1/NorAm_nival_glacial_basins_outlines.shp',\n",
      " 'output_data_path': '/Users/lla068/Documents/data_driven_forecasting/scripts/HPC_workflow/output_data/North_America/v1/',\n",
      " 'plots_path': '/Users/lla068/Documents/data_driven_forecasting/scripts/HPC_workflow/output_plots/North_America/v1/',\n",
      " 'precip_obs_path': '/Users/lla068/Documents/data_driven_forecasting/data/met_obs/North_America/SCDNA_v1.1.nc4',\n",
      " 'streamflow_obs_path': '/Users/lla068/Documents/data_driven_forecasting/data/streamflow_obs/North_America/NorAmQobs_1979_2021.nc',\n",
      " 'volumes_obs_path': '/Users/lla068/Documents/data_driven_forecasting/scripts/HPC_workflow/output_data/North_America/v1/NorAmVol_1979_2021_nival_glacial_basins.nc'}\n"
     ]
    }
   ],
   "source": [
    "settings = read_settings('../settings/config_North_America.yaml', log_settings=True)\n",
    "pprint(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set user-specified variables\n",
    "start_date = '1979-01-01' # start date for data extraction\n",
    "end_date = '2021-12-31' # end date for data extraction\n",
    "test_basin_id = '03339000' # USA test basin code from the USGS, for which we want to download test data\n",
    "\n",
    "logger.debug(f'start_date: {start_date}')\n",
    "logger.debug(f'end_date: {end_date}')\n",
    "logger.debug(f'test_basin_id: {test_basin_id}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore the basins:\n",
    "- in Canada: https://wateroffice.ec.gc.ca/map/index_e.html?type=historical\n",
    "- in the USA: https://maps.waterdata.usgs.gov/mapper/index.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamflow data download for USA test basin \n",
    "This section downloads USGS streamflow data using this [Python package](https://github.com/USGS-python/dataretrieval) and saves it into a NetCDF. Only stations with limited regulation, from the USGS Hydro-Climatic Data Network 2009 (HCDN–2009) ([Lins, 2012](https://pubs.usgs.gov/fs/2012/3047/)), are kept. The HCDN-2009 data are available [here](https://water.usgs.gov/GIS/metadata/usgswrd/XML/gagesII_Sept2011.xml)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decisions:\n",
    "- We extract data for 1979-2021 as this is when we have SWE data. [Brunner et al. (2020)](https://hess.copernicus.org/articles/24/3951/2020/) download data from 1981-2018 as data for this period were available for most stations in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import specific modules required for this sub-section\n",
    "import dataretrieval.nwis as nwis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set required data paths\n",
    "Q_netcdf_output = \"/Users/lla068/Documents/data_driven_forecasting/data/streamflow_obs/USGS_\"+test_site_id+\"_Qdata_\"+start_date+\"-\"+end_date+\".nc\" # NetCDF output path and file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get daily discharge data (in ft3/s)\n",
    "df = nwis.get_record(sites=test_basin_id, service='dv', start=start_date, end=end_date, parameterCd='00060')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the column \"00060_Mean_cd\", A stands for Approved for publication and P stands for Provisional data subject to revision. \n",
    "\n",
    "Some sites may measure from different locations, such as the right bank and the left bank of a river. Data for the different locations of a site will appear in separate columns - e.g., '00060_loc 1_Mean', '00060_loc 2_Mean', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this as NetCDF with right format\n",
    "# Download regulation dataset & shapefiles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWE data download for USA test basin\n",
    "Vincent Vionnet's script on GitHub?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamflow data download for Canada test basin\n",
    "Kasra & Shervan's script to download sqlite HYDAT data as csv files on GitHub?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the Canada HYDAT National Water Data Archive manually from [here](https://www.canada.ca/en/environment-climate-change/services/water-overview/quantity/monitoring/survey/data-products-services/national-archive-hydat.html). Click on \"Download the HYDAT database\". Once on that page, download:\n",
    "- the basin shapefiles by opening \"HydrometricNetworkBasinPolygons/\" and by downloading zip files \"01.zip\" to \"11.zip\" and unzip them. Each number corresponds to a different HYDAT region of Canada.\n",
    "- the regulation data by opening \"RHBN/\" and by downloading \"RHBN_Metadata.xlsx\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-format\n",
    "\n",
    "Here is the code in easymore to put the csv or data frame into a netcdf file.\n",
    "at least on csv file should be prepared that include the station information in this way\n",
    "https://github.com/ShervanGharari/EASYMORE/blob/main/data/station_data/station_data.csv\n",
    "\n",
    "if you have flags, this can be done in another file\n",
    "https://github.com/ShervanGharari/EASYMORE/blob/main/data/station_data/station_data_flag.csv\n",
    "\n",
    "if you have station information this can be done in another file as well! (which can have all the station information as many as you like)\n",
    "https://github.com/ShervanGharari/EASYMORE/blob/main/data/station_data/station_info.csv\n",
    "\n",
    "example is here:\n",
    "https://github.com/ShervanGharari/EASYMORE/blob/main/examples/Chapter1_E7.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWE data download for Canada test basin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the [Canadian historical Snow Water Equivalent dataset](https://zenodo.org/record/6638382) manually from Zenodo. Make sure to select the latest available version. You can find more information about this dataset in [Vionnet et al. (2021)](https://essd.copernicus.org/articles/13/4603/2021/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move below to HPC script"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All HCDN-2009 unregulated stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set required data paths\n",
    "hcdn_shp = '/Users/lla068/Documents/data_driven_forecasting/basins/USA/gagesII_9322_point_shapefile/gagesII_9322_sept30_2011.shp' # HCDN-2009 shapefile\n",
    "netcdf_output = \"/Users/lla068/Documents/data_driven_forecasting/data/USA/streamflow_obs/USGS_HCDN-2009_Qdata_1979-2021.nc\" # NetCDF output path and file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read HCDN-2009 shapefile as Geopandas dataframe\n",
    "hcdn_gdf = gpd.read_file(hcdn_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subselect data from the HCDN-2009 unregulated dataset\n",
    "hcdn_2009_gdf = hcdn_gdf.loc[hcdn_gdf['HCDN_2009'] == 'yes']\n",
    "\n",
    "display(hcdn_2009_gdf)\n",
    "\n",
    "hcdn_2009_sites = list(hcdn_2009_gdf.STAID.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract USGS Q data\n",
    "\n",
    "# loop over sites\n",
    "for s in hcdn_2009_sites:\n",
    "\n",
    "    # get daily discharge data (in ft3/s) for approved data only (00060_Mean_cd = A)\n",
    "    df = nwis.get_record(sites=s, service='dv', start=start_date, end=end_date, parameterCd='00060')\n",
    "\n",
    "    # check that station has data\n",
    "    if df.empty == False:\n",
    "    \n",
    "        # some sites may measure from different locations, such as the right bank and the left bank of a river (defined by loc)\n",
    "        if '00060_Mean_cd' in list(df.columns):\n",
    "            column_kwarg = '00060_Mean_cd'\n",
    "            column_data = '00060_Mean'\n",
    "        elif '00060_loc 1_Mean_cd' in list(df.columns):\n",
    "            column_kwarg = '00060_loc 1_Mean_cd'\n",
    "            column_data = '00060_loc 1_Mean'\n",
    "\n",
    "        # in the column \"00060_Mean_cd\", A stands for Approved for publication and P stands for Provisional data subject to revision\n",
    "        # we only keep the approved values\n",
    "        df = df.loc[df[column_kwarg] == 'A']\n",
    "\n",
    "        if s == hcdn_2009_sites[0]:\n",
    "            hcdn_2009_Q_df = df.loc[:,[column_data]]\n",
    "            hcdn_2009_Q_df = hcdn_2009_Q_df.rename(columns={column_data:s})\n",
    "\n",
    "        else:\n",
    "            df = df.rename(columns={column_data:s})\n",
    "            hcdn_2009_Q_df = pd.concat([hcdn_2009_Q_df, df[s]], axis=1)\n",
    "    \n",
    "    # if no data go to next station\n",
    "    else:\n",
    "        print(s,'has no data')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(hcdn_2009_Q_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change date format to datetime64[ns]\n",
    "hcdn_2009_Q_df.index = pd.to_datetime(hcdn_2009_Q_df.index)\n",
    "\n",
    "display(hcdn_2009_Q_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pandas dataframe to xarray DataArray\n",
    "hcdn_2009_Q_da = xr.DataArray(data=hcdn_2009_Q_df, coords=dict(time=hcdn_2009_Q_df.index.values, Station_ID=hcdn_2009_Q_df.columns.values), dims=['time','Station_ID'], name='Flow', attrs={'long_name':'Daily flow','units':'ft3/s','info':'Data extracted by Louise Arnal (USask) from USGS using https://github.com/USGS-python/dataretrieval. Only stations from the USGS Hydro-Climatic Data Network 2009 (HCDN–2009) were kept (Lins, 2012; https://pubs.usgs.gov/fs/2012/3047/)'})\n",
    "\n",
    "display(hcdn_2009_Q_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stations lat/lon information\n",
    "hcdn_2009_gdf_latlon = hcdn_2009_gdf[{'STAID','LAT_GAGE','LNG_GAGE'}].set_index('STAID')\n",
    "lats = hcdn_2009_gdf_latlon.loc[hcdn_2009_Q_da.Station_ID.values,'LAT_GAGE'].values\n",
    "lons = hcdn_2009_gdf_latlon.loc[hcdn_2009_Q_da.Station_ID.values,'LNG_GAGE'].values\n",
    "hcdn_2009_Q_da = hcdn_2009_Q_da.assign_coords(lat=(\"Station_ID\",lats),lon=(\"Station_ID\",lons))\n",
    "hcdn_2009_Q_da.lat.attrs['long_name'] = 'latitude'\n",
    "hcdn_2009_Q_da.lat.attrs['units'] = 'degrees_north'\n",
    "hcdn_2009_Q_da.lon.attrs['long_name'] = 'longitude'\n",
    "hcdn_2009_Q_da.lon.attrs['units'] = 'degrees_east'\n",
    "\n",
    "display(hcdn_2009_Q_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to NetCDF\n",
    "hcdn_2009_Q_da.to_netcdf(netcdf_output, format=\"NETCDF4\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All HYDAT basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYDAT_Q_path = \"/Users/lla068/Documents/data_driven_forecasting/data/streamflow_obs/Canada/HYDAT_sqlite3_20220418/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_list = sorted(os.listdir(HYDAT_Q_path))\n",
    "\n",
    "for x in stations_list:\n",
    "\n",
    "    df = pd.read_csv(HYDAT_Q_path+x, index_col=0)\n",
    "    \n",
    "    # change date format to datetime64[ns]\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Pandas dataframe to xarray DataSet\n",
    "    ds = df.to_xarray().rename({'index':'time','FLOW':'Flow','FLAG':'Flag'})\n",
    "    ds = ds.expand_dims(dim={'Station_ID':[x[0:-4]]}, axis=1)\n",
    "    ds.Flow.attrs['long_name'] = 'Daily flow'\n",
    "    ds.Flow.attrs['units'] = 'm3/s'\n",
    "    ds.Flow.attrs['info'] = 'Data extracted by Shervan Gharari (USask), using scripts from Kasra Keshavarz (USask), and reformatted by Louise Arnal (USask).'\n",
    "    ds.Flag.attrs['info'] = 'Flag attached to HYDAT data. A (Partial): calculation for daily data is made with incomplete daily record. B (Ice): ice cover observed at the time of measurement. D (dry): conditions of the river dry at the time of measurement. E (estimate): observation is an estimate only.'\n",
    "    \n",
    "    # merge datasets for all stations\n",
    "    if x == stations_list[0]:\n",
    "        HYDAT_Q_ds = ds\n",
    "    else:\n",
    "        HYDAT_Q_ds = xr.merge([HYDAT_Q_ds, ds])\n",
    "    \n",
    "display(HYDAT_Q_ds)\n",
    "    \n",
    "    # save to netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data for specific time range?\n",
    "# add lat/lon info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
